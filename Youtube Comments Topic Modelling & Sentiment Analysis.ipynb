{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make a list of characters\n",
    "import string\n",
    "\n",
    "# to do data cleaning and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# to do text mining, processing, and analysis\n",
    "import nltk\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.util import ngrams\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# to do data visualization\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "import googleapiclient.discovery\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from gsdmm import MovieGroupProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Provide your API key, Channel Id or Playlist Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\" # fill in your API key here\n",
    "\n",
    "channel_id = \"\" # fill in the channel Id here\n",
    "\n",
    "playlist_id = \"\" # fill in the playlist Id here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Scraping Video Data & Comments from a Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the videos data (id, title, upload date, views, likes, comments count)\n",
    "\n",
    "def get_channel_videos(api_key, channel_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    \n",
    "    try:\n",
    "        # Get the uploads playlist ID\n",
    "        channel_response = youtube.channels().list(\n",
    "            part='contentDetails',\n",
    "            id=channel_id\n",
    "        ).execute()\n",
    "        \n",
    "        uploads_playlist_id = channel_response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "        \n",
    "        videos = []\n",
    "        next_page_token = None\n",
    "        \n",
    "        while True:\n",
    "            playlist_response = youtube.playlistItems().list(\n",
    "                part='snippet',\n",
    "                playlistId=uploads_playlist_id,\n",
    "                maxResults=50,  # 50 is the maximum allowed\n",
    "                pageToken=next_page_token\n",
    "            ).execute()\n",
    "            \n",
    "            video_ids = [item['snippet']['resourceId']['videoId'] for item in playlist_response['items']]\n",
    "            \n",
    "            # Get video details including statistics and snippet for upload date\n",
    "            video_response = youtube.videos().list(\n",
    "                part='snippet,statistics',\n",
    "                id=','.join(video_ids)\n",
    "            ).execute()\n",
    "            \n",
    "            for item in video_response['items']:\n",
    "                video_id = item['id']\n",
    "                video_title = item['snippet']['title']\n",
    "                upload_date = item['snippet']['publishedAt']\n",
    "                stats = item['statistics']\n",
    "                videos.append({\n",
    "                    'video_id': video_id,\n",
    "                    'title': video_title,\n",
    "                    'url': f\"https://www.youtube.com/watch?v={video_id}\",\n",
    "                    'upload_date': upload_date,\n",
    "                    'views': int(stats.get('viewCount', 0)),\n",
    "                    'likes': int(stats.get('likeCount', 0)),\n",
    "                    'comments': int(stats.get('commentCount', 0)),\n",
    "                    'favorites': int(stats.get('favoriteCount', 0)),\n",
    "                    'dislikes': int(stats.get('dislikeCount', 0)) if 'dislikeCount' in stats else None\n",
    "                })\n",
    "            \n",
    "            next_page_token = playlist_response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "        \n",
    "        return videos\n",
    "    \n",
    "    except HttpError as e:\n",
    "        print(f'An HTTP error {e.resp.status} occurred:\\n{e.content}')\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve video data\n",
    "video_data = get_channel_videos(api_key, channel_id)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(video_data)\n",
    "\n",
    "# Save to Excel file\n",
    "df.to_excel(\"video data.xlsx\", index=False)\n",
    "\n",
    "# You can also choose different format like .csv \n",
    "# df.to_csv(\"video data.csv\", index=False)\n",
    "\n",
    "# Print a summary\n",
    "print(f\"Total videos retrieved: {len(df)}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the video comments \n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "DEVELOPER_KEY = api_key\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
    "\n",
    "def getcomments(video):\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video,\n",
    "            maxResults=100\n",
    "        )\n",
    "\n",
    "        comments = []\n",
    "\n",
    "        # Execute the request.\n",
    "        response = request.execute()\n",
    "\n",
    "        # Get the comments from the response.\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            public = item['snippet']['isPublic']\n",
    "            comments.append([\n",
    "                comment['authorDisplayName'],\n",
    "                comment['publishedAt'],\n",
    "                comment['likeCount'],\n",
    "                comment['textOriginal'],\n",
    "                comment['videoId'],\n",
    "                public\n",
    "            ])\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                nextPageToken = response['nextPageToken']\n",
    "            except KeyError:\n",
    "                break\n",
    "            \n",
    "            # Create a new request object with the next page token.\n",
    "            nextRequest = youtube.commentThreads().list(part=\"snippet\", videoId=video, maxResults=100, pageToken=nextPageToken)\n",
    "            # Execute the next request.\n",
    "            response = nextRequest.execute()\n",
    "            # Get the comments from the next response.\n",
    "            for item in response['items']:\n",
    "                comment = item['snippet']['topLevelComment']['snippet']\n",
    "                public = item['snippet']['isPublic']\n",
    "                comments.append([\n",
    "                    comment['authorDisplayName'],\n",
    "                    comment['publishedAt'],\n",
    "                    comment['likeCount'],\n",
    "                    comment['textOriginal'],\n",
    "                    comment['videoId'],\n",
    "                    public\n",
    "                ])\n",
    "\n",
    "        df2 = pd.DataFrame(comments, columns=['author', 'updated_at', 'like_count', 'text','video_id','public'])\n",
    "        return df2\n",
    "    except HttpError as e:\n",
    "        if 'commentsDisabled' in str(e):\n",
    "            print(f\"Comments are disabled for video ID: {video}\")\n",
    "            return pd.DataFrame(columns=['author', 'updated_at', 'like_count', 'text','video_id','public'])\n",
    "        else:\n",
    "            print(f\"An error occurred for video ID {video}: {str(e)}\")\n",
    "            return pd.DataFrame(columns=['author', 'updated_at', 'like_count', 'text','video_id','public'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ids = df['video_id'].to_list()\n",
    "print(f\"Number of videos: {len(video_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store all comment DataFrames\n",
    "all_comments = []\n",
    "\n",
    "# Iterate through video IDs and collect comments\n",
    "for video_id in video_ids:\n",
    "    comments_df = getcomments(video_id)\n",
    "    if not comments_df.empty:\n",
    "        all_comments.append(comments_df)\n",
    "\n",
    "# Combine all comment DataFrames into a single DataFrame\n",
    "df_all_comments = pd.concat(all_comments, ignore_index=True)\n",
    "\n",
    "# Now df_all_comments contains all comments from all videos\n",
    "print(f\"Total comments collected: {len(df_all_comments)}\")\n",
    "print(df_all_comments.head())\n",
    "\n",
    "# Optionally, save to an excel or csv\n",
    "df_all_comments.to_excel('all_comments.xlsx', index=False)\n",
    "print(\"All comments saved to 'all_comments.xlsx'\")\n",
    "\n",
    "df_all_comments.to_excel(\"all_youtube_comments.xlsx\")\n",
    "\n",
    "# Merge df with df_all_comments based on 'video_id'\n",
    "df_combined = pd.merge(\n",
    "    df_all_comments,  # DataFrame with comments\n",
    "    df[['video_id', 'upload_date', 'title', 'views']],  # DataFrame with video details\n",
    "    on='video_id',  # Merge on 'video_id'\n",
    "    how='left'  # Keep all rows from df_all_comments, matching from df\n",
    ")\n",
    "\n",
    "# Check the first few rows of the merged DataFrame\n",
    "print(df_combined.head())\n",
    "\n",
    "# Optionally, save the combined DataFrame to a .xlsx or .csv\n",
    "df_combined.to_excel('combined_comments_with_video_details.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Scraping Video Data and Comments from Playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>video_id</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Serunya Uji Adrenalin Di Jembatan Rajamandala ...</td>\n",
       "      <td>5gzTcUlCrOI</td>\n",
       "      <td>2024-01-08T05:58:49Z</td>\n",
       "      <td>3688</td>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Keindahan Curug Cikondang Tawarkan Liburan Asi...</td>\n",
       "      <td>XJJ-onCvVAI</td>\n",
       "      <td>2024-01-08T05:58:53Z</td>\n",
       "      <td>7815</td>\n",
       "      <td>89</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cuci Mata Di Curug Ngebul Gak Bikin Kecewa! | ...</td>\n",
       "      <td>m9DD1ZmcAaQ</td>\n",
       "      <td>2024-01-08T05:59:02Z</td>\n",
       "      <td>2856</td>\n",
       "      <td>46</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Asiknya Main Offroad Di Tengah Kebun Teh Cianj...</td>\n",
       "      <td>tnh8wJWDWyw</td>\n",
       "      <td>2024-01-08T05:58:57Z</td>\n",
       "      <td>2407</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Melihat Pemandangan Di Atas Dari Bukit Kasur |...</td>\n",
       "      <td>MR1fYtxO3yY</td>\n",
       "      <td>2024-01-29T08:47:56Z</td>\n",
       "      <td>1276</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Sekarang Berenang Di Curug Putri Carita Nih! |...</td>\n",
       "      <td>nUzy2eSEkG4</td>\n",
       "      <td>2024-05-06T10:09:00Z</td>\n",
       "      <td>1188</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Menikmati Bukit Loloan Yang Ada Di Lombok Utar...</td>\n",
       "      <td>on8MoYfWbHM</td>\n",
       "      <td>2024-05-28T09:39:43Z</td>\n",
       "      <td>396</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>Merasakan Kesegaran Air Terjun Jagir Di Banyuw...</td>\n",
       "      <td>p6CMX2n7y8M</td>\n",
       "      <td>2024-04-22T07:44:56Z</td>\n",
       "      <td>919</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Explore Banten! Kegiatan Alam Menanam Karang P...</td>\n",
       "      <td>sdoFYNBgCFY</td>\n",
       "      <td>2024-05-03T11:53:50Z</td>\n",
       "      <td>414</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>Menikmati Keindahan Laguna Laguna Di Pantai Te...</td>\n",
       "      <td>tE9tVwgbw34</td>\n",
       "      <td>2024-05-20T10:46:39Z</td>\n",
       "      <td>5085</td>\n",
       "      <td>102</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title     video_id  \\\n",
       "0    Serunya Uji Adrenalin Di Jembatan Rajamandala ...  5gzTcUlCrOI   \n",
       "1    Keindahan Curug Cikondang Tawarkan Liburan Asi...  XJJ-onCvVAI   \n",
       "2    Cuci Mata Di Curug Ngebul Gak Bikin Kecewa! | ...  m9DD1ZmcAaQ   \n",
       "3    Asiknya Main Offroad Di Tengah Kebun Teh Cianj...  tnh8wJWDWyw   \n",
       "4    Melihat Pemandangan Di Atas Dari Bukit Kasur |...  MR1fYtxO3yY   \n",
       "..                                                 ...          ...   \n",
       "203  Sekarang Berenang Di Curug Putri Carita Nih! |...  nUzy2eSEkG4   \n",
       "204  Menikmati Bukit Loloan Yang Ada Di Lombok Utar...  on8MoYfWbHM   \n",
       "205  Merasakan Kesegaran Air Terjun Jagir Di Banyuw...  p6CMX2n7y8M   \n",
       "206  Explore Banten! Kegiatan Alam Menanam Karang P...  sdoFYNBgCFY   \n",
       "207  Menikmati Keindahan Laguna Laguna Di Pantai Te...  tE9tVwgbw34   \n",
       "\n",
       "              upload_date views likes comments  \n",
       "0    2024-01-08T05:58:49Z  3688    39        7  \n",
       "1    2024-01-08T05:58:53Z  7815    89        5  \n",
       "2    2024-01-08T05:59:02Z  2856    46        9  \n",
       "3    2024-01-08T05:58:57Z  2407    33        2  \n",
       "4    2024-01-29T08:47:56Z  1276    16        1  \n",
       "..                    ...   ...   ...      ...  \n",
       "203  2024-05-06T10:09:00Z  1188    16        3  \n",
       "204  2024-05-28T09:39:43Z   396    13        1  \n",
       "205  2024-04-22T07:44:56Z   919    12        2  \n",
       "206  2024-05-03T11:53:50Z   414     4        0  \n",
       "207  2024-05-20T10:46:39Z  5085   102       11  \n",
       "\n",
       "[208 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to get videos from playlist and their details\n",
    "def get_videos_from_playlist(playlist_id):\n",
    "    videos = []\n",
    "\n",
    "    # Fetch the playlist items (videos)\n",
    "    request = youtube.playlistItems().list(\n",
    "        part='snippet',\n",
    "        playlistId=playlist_id,\n",
    "        maxResults=50  # Max is 50 per request\n",
    "    )\n",
    "\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "\n",
    "        video_ids = []\n",
    "        for item in response['items']:\n",
    "            video_id = item['snippet']['resourceId']['videoId']\n",
    "            video_ids.append(video_id)\n",
    "\n",
    "        # Fetch details (title, upload date, and engagement metrics) for each video\n",
    "        if video_ids:\n",
    "            video_details_request = youtube.videos().list(\n",
    "                part='snippet,contentDetails,statistics',\n",
    "                id=','.join(video_ids)\n",
    "            )\n",
    "            video_details_response = video_details_request.execute()\n",
    "\n",
    "            for video in video_details_response['items']:\n",
    "                video_data = {\n",
    "                    'title': video['snippet']['title'],\n",
    "                    'video_id': video['id'],\n",
    "                    'upload_date': video['snippet']['publishedAt'],\n",
    "                    'views': video['statistics'].get('viewCount', 'N/A'),\n",
    "                    'likes': video['statistics'].get('likeCount', 'N/A'),\n",
    "                    'comments': video['statistics'].get('commentCount', 'N/A'),\n",
    "                }\n",
    "                videos.append(video_data)\n",
    "\n",
    "        # Check if there's another page of results\n",
    "        request = youtube.playlistItems().list_next(request, response)\n",
    "\n",
    "    return videos\n",
    "\n",
    "# Example usage\n",
    "video_details = get_videos_from_playlist(playlist_id)\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "df_playlist = pd.DataFrame(video_details)\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df_playlist.to_excel('playlist_video data.xlsx', index=False)\n",
    "df_playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ids_playlist = df_playlist['video_id'].to_list()\n",
    "\n",
    "# Create an empty list to store all comment DataFrames\n",
    "all_comments = []\n",
    "\n",
    "# Iterate through video IDs and collect comments\n",
    "for video_id in video_ids_playlist:\n",
    "    comments_df = getcomments(video_id)\n",
    "    if not comments_df.empty:\n",
    "        all_comments.append(comments_df)\n",
    "\n",
    "# Combine all comment DataFrames into a single DataFrame\n",
    "df_all_comments_playlist = pd.concat(all_comments, ignore_index=True)\n",
    "\n",
    "# Now df_all_comments contains all comments from all videos\n",
    "print(f\"Total comments collected: {len(df_all_comments_playlist)}\")\n",
    "print(df_all_comments_playlist.head())\n",
    "\n",
    "# Optionally, save to CSV\n",
    "df_all_comments_playlist.to_excel('all_comments_playlist.xlsx', index=False)\n",
    "print(\"All comments saved to 'all_comments_playlist.xlsx'\")\n",
    "\n",
    "# Merge df_jejakpetualang with df_all_comments_playlist based on 'video_id'\n",
    "df_combined_playlist = pd.merge(\n",
    "    df_all_comments_playlist,  # DataFrame with comments\n",
    "    df_playlist[['video_id', 'upload_date', 'title', 'views']],  # DataFrame with video details\n",
    "    on='video_id',  # Merge on 'video_id'\n",
    "    how='left'  # Keep all rows from df_all_comments_playlist, matching from df_jejakpetualang\n",
    ")\n",
    "\n",
    "# Check the first few rows of the merged DataFrame\n",
    "print(df_combined_playlist.head())\n",
    "\n",
    "# Optionally, save the combined DataFrame to a CSV\n",
    "df_combined_playlist.to_excel('playlist_comments_with_video_details.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data Cleaning for Topic Modelling and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_words = pd.read_table(\"combined_slang_words.txt\")\n",
    "root_words = pd.read_table(\"combined_root_words.txt\")\n",
    "stop_words = pd.read_table(\"combined_stop_words.txt\")\n",
    "slang = pd.read_csv(\"colloquial-indonesian-lexicon.csv\")[['slang','formal','In-dictionary']]\n",
    "slang_dict = slang[['slang','formal']][slang['In-dictionary']==1]\n",
    "stop_words_list = [item for sublist in stop_words.values.tolist() for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adapun', 'agaknya', 'akan', 'akhir', 'akhirnya', 'akulah', 'amatlah', 'andalah', 'antara', 'apa', 'apabila', 'apalagi', 'artinya', 'asalkan', 'atau', 'ataupun', 'awalnya', 'bagaikan', 'bagaimanakah', 'bagi', 'bahkan', 'bahwasanya', 'bakal', 'balik', 'bapak', 'bawah', 'begini', 'beginikah', 'begitu', 'begitulah', 'bekerja', 'belakangan', 'belumlah', 'benarkah', 'berada', 'berakhirlah', 'berapa', 'berapalah', 'berarti', 'berbagai', 'beri', 'berikut', 'berjumlah', 'berkata', 'berkeinginan', 'berlainan', 'berlangsung', 'bermacam', 'bermaksud', 'bersama', 'bersiap', 'bertanya', 'berturut', 'bertutur', 'berupa', 'betul', 'biasa', 'bila', 'bisa', 'boleh', 'bolehlah', 'bukan', 'bukanlah', 'bulan', 'cara', 'cukup', 'cukuplah', 'dahulu', 'dan', 'dari', 'datang', 'demi', 'demikianlah', 'depan', 'dia', 'diakhirinya', 'diantara', 'diberi', 'diberikannya', 'dibuatnya', 'didatangkan', 'diibaratkan', 'diingat', 'diinginkan', 'dijelaskan', 'dikarenakan', 'dikatakannya', 'diketahui', 'dikira', 'dilalui', 'dimaksud', 'dimaksudkannya', 'diminta', 'dimisalkan', 'dimulailah', 'dimungkinkan', 'dipastikan', 'diperbuatnya', 'diperkirakan', 'diperlukan', 'dipersoalkan', 'dipunyai', 'dirinya', 'disebut', 'disebutkannya', 'disinilah', 'ditandaskan', 'ditanyai', 'ditegaskan', 'ditunjuk', 'ditunjukkan', 'ditunjuknya', 'dituturkannya', 'diucapkannya', 'dong', 'dulu', 'enggak', 'entah', 'guna', 'hal', 'hanya', 'hari', 'haruslah', 'hendak', 'hendaknya', 'ia', 'ibarat', 'ibaratnya', 'ikut', 'ingat-ingat', 'inginkah', 'ini', 'inilah', 'itukah', 'jadi', 'jadinya', 'jangankan', 'jauh', 'jawaban', 'jelas', 'jelaslah', 'jika', 'juga', 'jumlahnya', 'kala', 'kalaulah', 'kalian', 'kamilah', 'kamulah', 'kapan', 'kapanpun', 'karenanya', 'kata', 'katakanlah', 'ke', 'kebetulan', 'kedua', 'keinginan', 'kelihatan', 'kelima', 'kembali', 'kemungkinan', 'kenapa', 'kepadanya', 'keseluruhan', 'keterlaluan', 'khususnya', 'kinilah', 'kira-kira', 'kita', 'kok', 'lagi', 'lah', 'lainnya', 'lama', 'lanjut', 'lebih', 'lima', 'macam', 'makanya', 'malah', 'mampu', 'mana', 'manalagi', 'masalah', 'masih', 'masing', 'mau', 'melainkan', 'melalui', 'melihatnya', 'memastikan', 'memberikan', 'memerlukan', 'meminta', 'memisalkan', 'mempergunakan', 'memperlihatkan', 'mempersoalkan', 'mempunyai', 'memungkinkan', 'menambahkan', 'menanti', 'menantikan', 'menanyai', 'mendapat', 'mendatang', 'mendatangkan', 'mengakhiri', 'mengatakan', 'mengenai', 'mengetahui', 'menghendaki', 'mengibaratkannya', 'mengingatkan', 'mengira', 'mengucapkannya', 'menjadi', 'menjelaskan', 'menunjuk', 'menunjukkan', 'menurut', 'menyampaikan', 'menyatakan', 'menyeluruh', 'merasa', 'merekalah', 'meski', 'meyakini', 'minta', 'misal', 'misalnya', 'mulai', 'mulanya', 'mungkinkah', 'naik', 'nanti', 'nyaris', 'oleh', 'pada', 'padanya', 'paling', 'pantas', 'pasti', 'penting', 'per', 'perlu', 'perlunya', 'persoalan', 'pertama-tama', 'pertanyakan', 'pihaknya', 'pula', 'punya', 'rasanya', 'rupanya', 'saatnya', 'sajalah', 'sama', 'sambil', 'sampai-sampai', 'sana', 'sangatlah', 'saya', 'se', 'sebabnya', 'sebagaimana', 'sebagian', 'sebaik-baiknya', 'sebaliknya', 'sebegini', 'sebelum', 'sebenarnya', 'sebesar', 'sebisanya', 'sebut', 'sebutnya', 'secukupnya', 'sedangkan', 'sedikit', 'seenaknya', 'segalanya', 'seharusnya', 'seingat', 'sejauh', 'sejumlah', 'sekadarnya', 'sekali-kali', 'sekaligus', 'sekarang', 'sekecil', 'sekiranya', 'sekitarnya', 'sekurangnya', 'selain', 'selalu', 'selama-lamanya', 'selanjutnya', 'seluruhnya', 'semakin', 'semampunya', 'semasih', 'semata-mata', 'sementara', 'semisalnya', 'semua', 'semula', 'sendirian', 'seolah', 'seorang', 'sepantasnya', 'seperlunya', 'sepertinya', 'sering', 'serta', 'sesaat', 'sesampai', 'sesekali', 'sesuatu', 'sesudah', 'setelah', 'setengah', 'setiap', 'setibanya', 'setidaknya', 'seusai', 'siap', 'siapakah', 'sini', 'soal', 'suatu', 'sudahkah', 'supaya', 'tadinya', 'tahun', 'tambah', 'tampak', 'tandas', 'tanpa', 'tanyakan', 'tapi', 'tegasnya', 'tempat', 'tentang', 'tentulah', 'tepat', 'terasa', 'terdahulu', 'terdiri', 'terhadapnya', 'teringat-ingat', 'terjadilah', 'terkira', 'terlebih', 'termasuk', 'tersampaikan', 'tersebutlah', 'tertuju', 'terutama', 'tetapi', 'tiba', 'tidak', 'tidaklah', 'tinggi', 'tunjuk', 'tutur', 'ucap', 'ujar', 'umum', 'ungkap', 'untuk', 'usai', 'wah', 'waktu', 'walau', 'wong', 'yakin', 'yang', 'ada', 'adanya', 'anda', 'apakah', 'atas', 'akankah', 'antar', 'ataukah', 'bagaimana', 'baik', 'baru', 'belum', 'bagai', 'beginilah', 'bilakah', 'buat', 'bukankah', 'contohnya', 'cuma', 'dapat', 'daripada', 'di', 'dilakukan', 'dialah', 'dini', 'harus', 'hanyalah', 'harusnya', 'hendaklah', 'itu', 'itulah', 'ingin', 'inginkan', 'jangan', 'kalau', 'kali', 'kamu', 'karena', 'katanya', 'kemudian', 'ketika', 'kalaupun', 'kapankah', 'kiranya', 'langsung', 'luar', 'lamanya', 'mengapa', 'mereka', 'makin', 'manakala', 'masihkah', 'mungkinlah', 'nah', 'nantinya', 'paparnya', 'pernah', 'pun', 'pastilah', 'saja', 'saling', 'sampai', 'sebab', 'secara', 'sejak', 'sendiri', 'seperti', 'seseorang', 'sebagainya', 'sebanyak', 'sebegitu', 'sebetulnya', 'sedemikian', 'sedikitnya', 'sekali', 'seketika', 'selagi', 'semaunya', 'semuanya', 'sendirinya', 'sepanjang', 'sepantasnyalah', 'serupa', 'sesama', 'sesuatunya', 'sesudahnya', 'seterusnya', 'sewaktu', 'tandasnya', 'terhadap', 'tersebut', 'tetap', 'tadi', 'tentu', 'tentunya', 'tertentu', 'tidakkah', 'yaitu', 'waduh', 'wahai', 'wakil ', 'waktunya ', 'warga ', 'wib ', 'rt', 'no', 'nyatanya', 'olehnya', 'padahal', 'pak', 'panjang', 'para', 'pentingnya', 'percuma', 'perlukah', 'pertama', 'pertanyaan', 'pihak', 'pukul', 'rasa', 'rata', 'saat', 'sama-sama', 'sampaikan', 'sangat', 'satu', 'sayalah', 'sebagai', 'sebaik', 'sebaiknya', 'sebelumnya', 'seberapa', 'sebuah', 'sebutlah', 'sedang', 'segala', 'segera', 'sehingga', 'sejenak', 'sekadar', 'sekalian', 'sekalipun', 'sekitar', 'sekurang-kurangnya', 'sela', 'selaku', 'selama', 'selamanya', 'seluruh', 'semacam', 'semampu', 'semasa', 'semata', 'semisal', 'sempat', 'seolah-olah', 'sepihak', 'seringnya', 'sesegera', 'setempat', 'setiba', 'setidak-tidaknya', 'setinggi', 'siapa', 'siapapun', 'sinilah', 'soalnya', 'sudah', 'sudahlah', 'tahu', 'tak', 'tambahnya', 'tampaknya', 'tanya', 'tanyanya', 'tegas', 'telah', 'tengah', 'terakhir', 'terbanyak', 'terdapat', 'teringat', 'terjadi', 'terjadinya', 'terlalu', 'terlihat', 'ternyata', 'terus', 'tiap', 'tiba-tiba', 'tiga', 'toh', 'turut', 'tuturnya', 'ucapnya', 'ujarnya', 'umumnya', 'ungkapnya', 'usah', 'waktunya', 'walaupun', 'yakni', 'dengan', 'orang', 'bahwa', 'namun', 'dua', 'kepada', 'lalu', 'lain', 'banyak', 'beberapa', 'besar', 'merupakan', 'agar', 'persen', 'wib', 'diri', 'minggu', 'the', 'selasa', 'jumlah', 'kondisi', 'hubungan', 'acara', 'masa', 'hidup', 'senin', 'maupun', 'mantan', 'jenis', 'juni', 'tinggal', 'asal', 'sesuai', 'berat', 'memberi', 'sabtu', 'mencari', 'ruang', 'biasanya', 'berdasarkan', 'pekan', 'membawa', 'tingkat', 'dekat', 'ketiga', 'ribu', 'membantu', 'khusus', 'ditemukan', 'kegiatan', 'tampil', 'bertemu', 'justru', 'menyebutkan', 'milik', 'menjalani', 'sumber', 'upaya', 'mengambil', 'lewat', 'meningkatkan', 'kehidupan', 'penggunaan', 'menghadapi', 'aku', 'kami', 'beliau', 'lo', 'lu', 'kemana', 'gimana', 'kurang', 'kemarin', 'yg', 'ya', 'emang', 'gitu', 'amat', 'sekian', 'sekedar', '&', 'telanjur', 'dimana', 'kah', 'kecuali', 'seraya', 'dsb', 'dll', 'dulunya', 'demikian', 'nggak', 'oh', 'tolong', 'bagaimanapun']\n"
     ]
    }
   ],
   "source": [
    "# Initialize Sastrawi (for Bahasa Indonesia) stemmer and stopword remover\n",
    "stop_factory = StopWordRemoverFactory()\n",
    "more_stopword = ['dengan', 'ia', 'bahwa', 'oleh', 'aja', 'kalau', 'kalo', 'yg', 'jadi','nya','sy',\n",
    "                 'yg','ustad','ustadz','ada','sama','semua','haji', 'utk', 'DAN', 'km', 'kmu', 'ak', \n",
    "                 'aq', 'akuh', 'dri', 'ada', 'ad', 'di'] # you could add more if you'd like to\n",
    "stopwords = stop_factory.get_stop_words() + more_stopword + stop_words_list\n",
    "stopwords = [i for i in stopwords if i != 'tidak']\n",
    "stopword_remover = stop_factory.create_stop_word_remover()\n",
    "\n",
    "stem_factory = StemmerFactory()\n",
    "stemmer = stem_factory.create_stemmer()\n",
    "\n",
    "# Preprocessing functions\n",
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_number(text):\n",
    "    return ''.join(char for char in text if not char.isdigit())\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punctuation = string.punctuation\n",
    "    return ''.join(char for char in text if char not in punctuation)\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join(word for word in nltk.word_tokenize(text) if word not in stopwords)\n",
    "\n",
    "def get_bigrams(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    bigram_phrases = [' '.join(bigram) for bigram in bigrams(tokens)]\n",
    "    return bigram_phrases\n",
    "\n",
    "def get_trigrams(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    trigram_phrases = [' '.join(trigram) for trigram in trigrams(tokens)]\n",
    "    return trigram_phrases\n",
    "\n",
    "def get_fourgrams(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    fourgram_phrases = [' '.join(fourgram) for fourgram in ngrams(tokens, 4)]\n",
    "    return fourgram_phrases\n",
    "\n",
    "def safe_preprocess(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    return text\n",
    "\n",
    "def replace_slang(text):\n",
    "    for row, slang, formal in slang_dict.itertuples():\n",
    "        text = text.replace(slang, formal)\n",
    "    return text\n",
    "\n",
    "def text_preprocessing(df, column):\n",
    "    # First, safely convert all values to strings and handle NaN\n",
    "    df[f'{column}_processed'] = df[column].apply(safe_preprocess)\n",
    "    \n",
    "    df[f'text_remove_number'] = df[f'{column}_processed'].apply(remove_number)\n",
    "    df[f'text_lower'] = df[f'text_remove_number'].apply(to_lower)\n",
    "    df[f'text_remove_punctuation'] = df[f'text_lower'].apply(remove_punctuation)\n",
    "    df[f'text_remove_stopwords'] = df[f'text_remove_punctuation'].apply(remove_stopwords)\n",
    "    df[f'text_stemmed'] = df[f'text_remove_stopwords'].apply(stemmer.stem)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "# Assuming 'df' is your DataFrame and 'text' is the column with comments\n",
    "\n",
    "# IMPORTANT\n",
    "# only choose one option and delete the other based on your source (channelId or playlistId)\n",
    "\n",
    "df_cleaned = text_preprocessing(df, \"text\") # this works if you use channel ID\n",
    "\n",
    "# or use this if you use playlist\n",
    "\n",
    "df_cleaned = text_preprocessing(df_combined_playlist, \"text\") # this works if you use playlist ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Topic Modelling and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rahmani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load your DataFrame\n",
    "# df = pd.read_csv('your_data.csv')  # Uncomment and modify this line to load your data\n",
    "\n",
    "# Assuming df has a 'text_stemmed' column with preprocessed text\n",
    "\n",
    "# Create a simple sentiment lexicon for Bahasa Indonesia\n",
    "# This is a very basic lexicon and should be expanded for better results\n",
    "\n",
    "positive_texts = pd.read_table(\"positive.txt\", names=[\"words\"])\n",
    "positive_words = [item for sublist in positive_texts.values.tolist() for item in sublist]\n",
    "\n",
    "negative_texts = pd.read_table(\"negative.txt\", names =[\"words\"])\n",
    "negative_words = [item for sublist in negative_texts.values.tolist() for item in sublist]\n",
    "\n",
    "# Step 2: TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_cleaned['text_stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate sentiment scores\n",
    "def get_sentiment_score(text, tfidf_vector):\n",
    "    words = word_tokenize(text.lower())\n",
    "    score = 0\n",
    "    for word in words:\n",
    "        if word in positive_words:\n",
    "            score += tfidf_vector[tfidf_vectorizer.vocabulary_[word]] if word in tfidf_vectorizer.vocabulary_ else 1\n",
    "        elif word in negative_words:\n",
    "            score -= tfidf_vector[tfidf_vectorizer.vocabulary_[word]] if word in tfidf_vectorizer.vocabulary_ else 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "df_cleaned['sentiment_score'] = df_cleaned.apply(\n",
    "    lambda row: get_sentiment_score(row['text_stemmed'], tfidf_matrix[row.name].toarray()[0]), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize sentiment scores\n",
    "df_cleaned['sentiment_score_normalized'] = 2 * (df_cleaned['sentiment_score'] - df_cleaned['sentiment_score'].min()) / (df_cleaned['sentiment_score'].max() - df_cleaned['sentiment_score'].min()) - 1\n",
    "\n",
    "# Classify sentiments based on scores\n",
    "def classify_sentiment(score):\n",
    "    if score < -0.1:\n",
    "        return 'negative'\n",
    "    elif score > 0.1:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "df_cleaned['sentiment_classification'] = df_cleaned['sentiment_score'].apply(classify_sentiment)\n",
    "\n",
    "df_cleaned.to_excel(\"sentiment ready comments.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 755 clusters with 15 clusters populated\n",
      "In stage 1: transferred 622 clusters with 15 clusters populated\n",
      "In stage 2: transferred 548 clusters with 15 clusters populated\n",
      "In stage 3: transferred 532 clusters with 14 clusters populated\n",
      "In stage 4: transferred 481 clusters with 14 clusters populated\n",
      "In stage 5: transferred 471 clusters with 14 clusters populated\n",
      "In stage 6: transferred 472 clusters with 14 clusters populated\n",
      "In stage 7: transferred 456 clusters with 14 clusters populated\n",
      "In stage 8: transferred 440 clusters with 14 clusters populated\n",
      "In stage 9: transferred 432 clusters with 14 clusters populated\n",
      "In stage 10: transferred 452 clusters with 14 clusters populated\n",
      "In stage 11: transferred 469 clusters with 14 clusters populated\n",
      "In stage 12: transferred 428 clusters with 14 clusters populated\n",
      "In stage 13: transferred 391 clusters with 14 clusters populated\n",
      "In stage 14: transferred 383 clusters with 14 clusters populated\n",
      "In stage 15: transferred 399 clusters with 13 clusters populated\n",
      "In stage 16: transferred 396 clusters with 13 clusters populated\n",
      "In stage 17: transferred 391 clusters with 13 clusters populated\n",
      "In stage 18: transferred 418 clusters with 13 clusters populated\n",
      "In stage 19: transferred 399 clusters with 14 clusters populated\n",
      "In stage 20: transferred 392 clusters with 13 clusters populated\n",
      "In stage 21: transferred 380 clusters with 13 clusters populated\n",
      "In stage 22: transferred 410 clusters with 14 clusters populated\n",
      "In stage 23: transferred 414 clusters with 13 clusters populated\n",
      "In stage 24: transferred 397 clusters with 13 clusters populated\n",
      "In stage 25: transferred 403 clusters with 13 clusters populated\n",
      "In stage 26: transferred 409 clusters with 14 clusters populated\n",
      "In stage 27: transferred 412 clusters with 14 clusters populated\n",
      "In stage 28: transferred 398 clusters with 14 clusters populated\n",
      "In stage 29: transferred 392 clusters with 14 clusters populated\n",
      "Top 5 words for each topic:\n",
      "Topic 0: indonesia, java, kelor, rubel, cepat, senyap, zerro, great, asia, pacific\n",
      "Topic 1: cowok, ganteng, gayaupil, hidung, sih, lupa, yah, trans, efek, ganti\n",
      "Topic 2: makan, kikil, sapi, olah, air, bikin, nama, masak, ayo, semangat\n",
      "Topic 3: mantap, hadir, sukses, salam, keren, kak, bang, semangat, nyimak, mas\n",
      "Topic 5: sdh, gak, buah, matang, mbak, alhamdulillah, jatuh, gan, bos, dee\n",
      "Topic 6: gan, piknik, bolong, cenut, ucap, ngan, sdap, coba, dongsht, sellu\n",
      "Topic 7: rumah, burung, tani, gampang, nangkap, puyuh, mantapppp, urang, mah, digoren\n",
      "Topic 8: main, maen, hee, semangt, kalw, kacangnyahblm, ambil, miyaknyah, cantik, luntur\n",
      "Topic 9: salam, rangga, keren, kaleng, mesan, bibit, kentang, hoyong, ngadamel, bagus\n",
      "Topic 10: slm, udah, sehat, milik, kebun, mas, loh, lari, sengat, madu\n",
      "Topic 11: anak, jejak, jjk, negeri, lahir, tanah, mantap, memang, tualang, tayang\n",
      "Topic 12: ikan, mantap, nama, gede, makan, kuning, mas, kuah, kan, cumi\n",
      "Topic 13: enak, banget, bikin, kang, puasa, sambal, mantul, ngiler, gak, daerah\n",
      "Topic 14: allah, tidak, manusia, jadi, sungguh, buat, kuasa, alquran, ingkar, orangorang\n",
      "Sample comments with their dominant topics:\n",
      "Preprocessed text: Bonwareng lorrr...\n",
      "Tokens: bonwareng lorrr...\n",
      "Dominant Topic: 6\n",
      "Topic Probability: 0.44528076422837887\n",
      "\n",
      "Preprocessed text: Keren ini mah dapet resep resep makanan...\n",
      "Tokens: keren mah dapet resep resep makan...\n",
      "Dominant Topic: 2\n",
      "Topic Probability: 0.9958943949629673\n",
      "\n",
      "Preprocessed text: Wehhh kak koriii✨✨✨...\n",
      "Tokens: wehhh kak koriii...\n",
      "Dominant Topic: 3\n",
      "Topic Probability: 0.9795930170121319\n",
      "\n",
      "Preprocessed text: Masakan tradisional yg sangat wuenakkkk.. sukses t...\n",
      "Tokens: masakan tradisional wuenakkkk sukses truss kakak...\n",
      "Dominant Topic: 11\n",
      "Topic Probability: 0.992144965349554\n",
      "\n",
      "Preprocessed text: Digoreng dibikin sambal enak...\n",
      "Tokens: goreng bikin sambal enak...\n",
      "Dominant Topic: 13\n",
      "Topic Probability: 0.9969883899729345\n",
      "\n",
      "\n",
      "Summary of topics:\n",
      "    dominant_topic  avg_probability  comment_count\n",
      "0                0         0.782954             16\n",
      "1                1         0.855876             13\n",
      "2                2         0.791316             56\n",
      "3                3         0.840368            433\n",
      "4                5         0.835180             23\n",
      "5                6         0.840599              7\n",
      "6                7         0.868043             11\n",
      "7                8         0.872715             10\n",
      "8                9         0.814124             39\n",
      "9               10         0.848244             10\n",
      "10              11         0.837753             67\n",
      "11              12         0.893326            100\n",
      "12              13         0.842966             71\n",
      "Results saved to 'JAN_comments_with_topics_gsdmm.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load your data (assuming 'text' column contains preprocessed text)\n",
    "df = pd.read_excel(\"sentiment ready comments.xlsx\")\n",
    "\n",
    "# Convert preprocessed text to list of tokens\n",
    "def text_to_tokens(text):\n",
    "    if isinstance(text, str):\n",
    "        return [word for word in text.split() if len(word) > 2]\n",
    "    return []\n",
    "\n",
    "# Apply tokenization to preprocessed comments\n",
    "df['tokens'] = df['text_stemmed'].apply(text_to_tokens)\n",
    "\n",
    "# Remove empty lists\n",
    "df = df[df['tokens'].map(len) > 0]\n",
    "\n",
    "# Prepare data for GSDMM\n",
    "# GSDMM requires a list of lists, where each inner list is the tokenized document\n",
    "documents = df['tokens'].tolist()\n",
    "\n",
    "# Initialize GSDMM\n",
    "num_topics = 15 # Number of clusters/topics\n",
    "gsdmm_model = MovieGroupProcess(K=num_topics, alpha=0.1, beta=0.3, n_iters=30)\n",
    "\n",
    "# Fit the GSDMM model on the tokenized documents\n",
    "vocab_length = len(set([word for doc in documents for word in doc]))\n",
    "gsdmm_model.fit(documents, vocab_length)\n",
    "\n",
    "# Assign topics to comments\n",
    "df['dominant_topic'] = df['tokens'].apply(lambda x: gsdmm_model.choose_best_label(x)[0])\n",
    "df['topic_probability'] = df['tokens'].apply(lambda x: gsdmm_model.choose_best_label(x)[1])\n",
    "\n",
    "# Print the topics\n",
    "print(\"Top 5 words for each topic:\")\n",
    "for topic_id, word_distribution in enumerate(gsdmm_model.cluster_word_distribution):\n",
    "    if len(word_distribution) == 0:  # Skip empty topics\n",
    "        continue\n",
    "    top_words = sorted(word_distribution.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    top_words = [word for word, freq in top_words]\n",
    "    print(f\"Topic {topic_id}: {', '.join(top_words)}\")\n",
    "\n",
    "# Print sample results\n",
    "print(\"Sample comments with their dominant topics:\")\n",
    "for _, row in df.sample(min(5, len(df))).iterrows():\n",
    "    print(f\"Preprocessed text: {row['text'][:50]}...\")\n",
    "    print(f\"Tokens: {' '.join(row['tokens'])[:50]}...\")\n",
    "    print(f\"Dominant Topic: {row['dominant_topic']}\")\n",
    "    print(f\"Topic Probability: {row['topic_probability']}\")\n",
    "    print()\n",
    "\n",
    "# Optionally, group by dominant topic and show average probability\n",
    "topic_summary = df.groupby('dominant_topic').agg({\n",
    "    'topic_probability': ['mean', 'count']\n",
    "}).reset_index()\n",
    "topic_summary.columns = ['dominant_topic', 'avg_probability', 'comment_count']\n",
    "print(\"\\nSummary of topics:\")\n",
    "print(topic_summary)\n",
    "\n",
    "# Optionally, save results\n",
    "df.to_csv('comments_with_topics_gsdmm.csv', index=False)\n",
    "print(\"Results saved to 'comments_with_topics_gsdmm.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note:\n",
    "\n",
    "- Click on scrollable element to see the description of the topics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
